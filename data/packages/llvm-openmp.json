{
    "name": "llvm-openmp",
    "aliases": [],
    "versions": [
        {
            "name": "9.0.0",
            "sha256": "9979eb1133066376cc0be29d1682bc0b0e7fb541075b391061679111ae4d3b5b"
        },
        {
            "name": "8.0.0",
            "sha256": "f7b1705d2f16c4fc23d6531f67d2dd6fb78a077dd346b02fed64f4b8df65c9d5"
        }
    ],
    "build_system": "CMakePackage",
    "conflicts": [
        {
            "name": "+ipo",
            "spec": "^cmake@:3.8",
            "description": "+ipo is not supported by CMake < 3.9"
        }
    ],
    "variants": [
        {
            "name": "build_type",
            "default": "RelWithDebInfo",
            "description": "CMake build type"
        },
        {
            "name": "ipo",
            "default": false,
            "description": "CMake interprocedural optimization"
        },
        {
            "name": "multicompat",
            "default": false,
            "description": "Support gomp and the Intel openMP runtime library."
        }
    ],
    "homepage": "https://openmp.llvm.org/",
    "maintainers": [],
    "patches": [],
    "resources": [],
    "description": "The OpenMP subproject of LLVM contains the components required to build\nan executable OpenMP program that are outside the compiler itself.\n",
    "dependencies": [
        {
            "name": "cmake",
            "description": "A cross-platform, open-source build system. CMake is a family of tools\ndesigned to build, test and package software."
        }
    ],
    "dependent_to": [
        {
            "name": "lbann",
            "description": "LBANN: Livermore Big Artificial Neural Network Toolkit. A distributed\nmemory, HPC-optimized, model and data parallel training toolkit for deep\nneural networks."
        },
        {
            "name": "py-xgboost",
            "description": "XGBoost is an optimized distributed gradient boosting library designed\nto be highly efficient, flexible and portable."
        },
        {
            "name": "py-dgl",
            "description": "Deep Graph Library (DGL). DGL is an easy-to-use, high performance and\nscalable Python package for deep learning on graphs. DGL is framework\nagnostic, meaning if a deep graph model is a component of an end-to-end\napplication, the rest of the logics can be implemented in any major\nframeworks, such as PyTorch, Apache MXNet or TensorFlow."
        },
        {
            "name": "py-torch",
            "description": "Tensors and Dynamic neural networks in Python with strong GPU\nacceleration."
        },
        {
            "name": "fbgemm",
            "description": "FBGEMM (Facebook GEneral Matrix Multiplication) is a low-precision,\nhigh-performance matrix-matrix multiplications and convolution library\nfor server-side inference."
        },
        {
            "name": "py-scikit-learn",
            "description": "A set of python modules for machine learning and data mining."
        },
        {
            "name": "opensubdiv",
            "description": "OpenSubdiv is a set of open source libraries that implement high\nperformance subdivision surface (subdiv) evaluation on massively\nparallel CPU and GPU architectures. This code path is optimized for\ndrawing deforming surfaces with static topology at interactive\nframerates."
        },
        {
            "name": "hipace",
            "description": "Highly efficient Plasma Accelerator Emulation, quasistatic particle-in-\ncell code"
        },
        {
            "name": "onednn",
            "description": "oneAPI Deep Neural Network Library (oneDNN). Formerly known as Intel\nMKL-DNN and DNNL."
        },
        {
            "name": "fujitsu-fftw",
            "description": "FFTW (Fujitsu Optimized version) is a comprehensive collection of fast C\nroutines for computing the Discrete Fourier Transform (DFT) and various\nspecial cases thereof. It is an open-source implementation of the Fast\nFourier transform algorithm. It can compute transforms of real and\ncomplex-values arrays of arbitrary size and dimension. Fujitsu Optimized\nFFTW is the optimized FFTW implementation targeted for A64FX CPUs. For\nsingle precision build, please use precision value as float. Example :\nspack install fujitsufftw precision=float"
        },
        {
            "name": "amdfftw",
            "description": "FFTW (AMD Optimized version) is a comprehensive collection of fast C\nroutines for computing the Discrete Fourier Transform (DFT) and various\nspecial cases thereof. It is an open-source implementation of the Fast\nFourier transform algorithm. It can compute transforms of real and\ncomplex-values arrays of arbitrary size and dimension. AMD Optimized\nFFTW is the optimized FFTW implementation targeted for AMD CPUs. For\nsingle precision build, please use precision value as float. Example :\nspack install amdfftw precision=float"
        },
        {
            "name": "warpx",
            "description": "WarpX is an advanced electromagnetic Particle-In-Cell code. It supports\nmany features including Perfectly-Matched Layers (PML) and mesh\nrefinement. In addition, WarpX is a highly-parallel and highly-optimized\ncode and features hybrid OpenMP/MPI parallelization, advanced\nvectorization techniques and load balancing capabilities. For WarpX'\nPython bindings and PICMI input support, see the 'py-warpx' package."
        },
        {
            "name": "dihydrogen",
            "description": "DiHydrogen is the second version of the Hydrogen fork of the well-known\ndistributed linear algebra library, Elemental. DiHydrogen aims to be a\nbasic distributed multilinear algebra interface with a particular\nemphasis on the needs of the distributed machine learning effort, LBANN."
        },
        {
            "name": "hydrogen",
            "description": "Hydrogen: Distributed-memory dense and sparse-direct linear algebra and\noptimization library. Based on the Elemental library."
        },
        {
            "name": "fftw",
            "description": "FFTW is a C subroutine library for computing the discrete Fourier\ntransform (DFT) in one or more dimensions, of arbitrary input size, and\nof both real and complex data (as well as of even/odd data, i.e. the\ndiscrete cosine/sine transforms or DCT/DST). We believe that FFTW, which\nis free software, should become the FFT library of choice for most\napplications."
        },
        {
            "name": "blaspp",
            "description": "C++ API for the Basic Linear Algebra Subroutines. Developed by the\nInnovative Computing Laboratory at the University of Tennessee,\nKnoxville."
        },
        {
            "name": "xgboost",
            "description": "XGBoost is an optimized distributed gradient boosting library designed\nto be highly efficient, flexible and portable. It implements machine\nlearning algorithms under the Gradient Boosting framework. XGBoost\nprovides a parallel tree boosting (also known as GBDT, GBM) that solve\nmany data science problems in a fast and accurate way. The same code\nruns on major distributed environment (Hadoop, SGE, MPI) and can solve\nproblems beyond billions of examples."
        },
        {
            "name": "py-networkit",
            "description": "NetworKit is a growing open-source toolkit for large-scale network\nanalysis. Its aim is to provide tools for the analysis of large networks\nin the size range from thousands to billions of edges. For this purpose,\nit implements efficient graph algorithms, many of them parallel to\nutilize multicore architectures. These are meant to compute standard\nmeasures of network analysis, such as degree sequences, clustering\ncoefficients, and centrality measures. In this respect, NetworKit is\ncomparable to packages such as NetworkX, albeit with a focus on\nparallelism and scalability."
        }
    ]
}